{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuananhvienngoc/list-spatial-web-service-examples/blob/master/GeoSAM-Image-Encoder/examples/geosam-image-encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GeoSAM-Image-Encoder (Python package)\n",
        "\n",
        "[![PyPI Version](https://img.shields.io/pypi/v/GeoSAM-Image-Encoder)](https://pypi.org/project/GeoSAM-Image-Encoder)\n",
        "[![Downloads](https://static.pepy.tech/badge/GeoSAM-Image-Encoder)](https://pepy.tech/project/GeoSAM-Image-Encoder)\n",
        "\n",
        "\n",
        "This package is part of the [Geo-SAM](https://github.com/coolzhao/Geo-SAM) project and is a standalone Python package that does not depend on QGIS. This package allows you to **encode remote sensing images into features that can be recognized by Geo-SAM using a remote server**, such as ``Colab``, ``AWS``, ``Azure`` or your own ``HPC``."
      ],
      "metadata": {
        "id": "ofyKNeqP6Wj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "Installing `GeoSAM-Image-Encoder` may directly install the CPU version of `PyTorch`. Therefore, it is recommended to install the appropriate version of `PyTorch` before installing `GeoSAM-Image-Encoder` in your machine. You can install the corresponding version based on the official PyTorch website:\n",
        "<https://pytorch.org/get-started/locally/>\n",
        "\n",
        "After installing PyTorch, you can install `GeoSAM-Image-Encoder` via pip.\n"
      ],
      "metadata": {
        "id": "yd7SYS1o6NTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Colab, PyTorch is already built-in, so you can install it directly."
      ],
      "metadata": {
        "id": "dM0YGfCtsfUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x_LOrmGt3Gkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72004bfc-f19c-43e0-9574-1cb698cc3692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GeoSAM-Image-Encoder\n",
            "  Downloading GeoSAM_Image_Encoder-1.0.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from GeoSAM-Image-Encoder) (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from GeoSAM-Image-Encoder) (4.67.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from GeoSAM-Image-Encoder) (75.2.0)\n",
            "Collecting torchgeo (from GeoSAM-Image-Encoder)\n",
            "  Downloading torchgeo-0.8.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting segment-anything (from GeoSAM-Image-Encoder)\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->GeoSAM-Image-Encoder) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->GeoSAM-Image-Encoder) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->GeoSAM-Image-Encoder) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->GeoSAM-Image-Encoder) (2025.3)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (0.8.1)\n",
            "Requirement already satisfied: geopandas>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (1.1.1)\n",
            "Collecting jsonargparse>=4.25 (from jsonargparse[signatures]>=4.25->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading jsonargparse-4.44.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting kornia>=0.8.2 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading kornia-0.8.2-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting lightly!=1.4.26,>=1.4.5 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightning-2.6.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (3.10.0)\n",
            "Requirement already satisfied: pillow>=9.2 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (11.3.0)\n",
            "Requirement already satisfied: pyproj>=3.4 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (3.7.2)\n",
            "Requirement already satisfied: rasterio>=1.4.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (1.4.4)\n",
            "Collecting segmentation-models-pytorch>=0.5 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: shapely>=2 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (2.1.2)\n",
            "Requirement already satisfied: timm>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (1.0.22)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (2.9.0+cu126)\n",
            "Collecting torchmetrics>=1.2 (from torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (0.24.0+cu126)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from torchgeo->GeoSAM-Image-Encoder) (4.15.0)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.12.1->torchgeo->GeoSAM-Image-Encoder) (0.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from geopandas>=0.12.1->torchgeo->GeoSAM-Image-Encoder) (25.0)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.12/dist-packages (from jsonargparse>=4.25->jsonargparse[signatures]>=4.25->torchgeo->GeoSAM-Image-Encoder) (6.0.3)\n",
            "Requirement already satisfied: docstring-parser>=0.17 in /usr/local/lib/python3.12/dist-packages (from jsonargparse[signatures]>=4.25->torchgeo->GeoSAM-Image-Encoder) (0.17.0)\n",
            "Collecting typeshed-client>=2.8.2 (from jsonargparse[signatures]>=4.25->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading typeshed_client-2.8.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia>=0.8.2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2025.11.12)\n",
            "Collecting hydra-core>=1.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lightly_utils~=0.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.32.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (1.17.0)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.12.3)\n",
            "Collecting pytorch_lightning>=1.0.4 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.5.0)\n",
            "Collecting aenum>=3.1.11 (from lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: fsspec<2027.0,>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (2025.3.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo->GeoSAM-Image-Encoder) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo->GeoSAM-Image-Encoder) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo->GeoSAM-Image-Encoder) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo->GeoSAM-Image-Encoder) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6->torchgeo->GeoSAM-Image-Encoder) (3.2.5)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo->GeoSAM-Image-Encoder) (2.4.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo->GeoSAM-Image-Encoder) (25.4.0)\n",
            "Requirement already satisfied: click!=8.2.*,>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo->GeoSAM-Image-Encoder) (8.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo->GeoSAM-Image-Encoder) (0.7.2)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.12/dist-packages (from rasterio>=1.4.3->torchgeo->GeoSAM-Image-Encoder) (1.1.1.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch>=0.5->torchgeo->GeoSAM-Image-Encoder) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch>=0.5->torchgeo->GeoSAM-Image-Encoder) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.5.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch>=0.5->torchgeo->GeoSAM-Image-Encoder) (1.2.0)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (4.9.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo->GeoSAM-Image-Encoder) (3.11)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2->torchgeo->GeoSAM-Image-Encoder) (1.3.0)\n",
            "Requirement already satisfied: importlib_resources>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from typeshed-client>=2.8.2->jsonargparse[signatures]>=4.25->torchgeo->GeoSAM-Image-Encoder) (6.5.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2->torchgeo->GeoSAM-Image-Encoder) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2027.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,!=2.5.5,!=2.5.6,>=2->torchgeo->GeoSAM-Image-Encoder) (1.22.0)\n",
            "Downloading GeoSAM_Image_Encoder-1.0.4-py3-none-any.whl (27 kB)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Downloading torchgeo-0.8.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.44.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.2-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly-1.5.22-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.6.0-py3-none-any.whl (845 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m119.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.8.2-py3-none-any.whl (760 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segment-anything, aenum, typeshed-client, lightning-utilities, lightly_utils, kornia_rs, jsonargparse, hydra-core, torchmetrics, kornia, pytorch_lightning, segmentation-models-pytorch, lightning, lightly, torchgeo, GeoSAM-Image-Encoder\n",
            "Successfully installed GeoSAM-Image-Encoder-1.0.4 aenum-3.1.16 hydra-core-1.3.2 jsonargparse-4.44.0 kornia-0.8.2 kornia_rs-0.1.10 lightly-1.5.22 lightly_utils-0.0.2 lightning-2.6.0 lightning-utilities-0.15.2 pytorch_lightning-2.6.0 segment-anything-1.0 segmentation-models-pytorch-0.5.0 torchgeo-0.8.0 torchmetrics-1.8.2 typeshed-client-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install GeoSAM-Image-Encoder\n",
        "# or\n",
        "# !pip install git+https://github.com/Fanchengyan/GeoSAM-Image-Encoder.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download example dataset and sam `vit_l` checkpoint"
      ],
      "metadata": {
        "id": "dM9ztEAm62Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/rasters/beiluhe_google_img_201211_clip.tif\n",
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
        "!wget https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/GeoSAM-Image-Encoder/examples/data/setting.json"
      ],
      "metadata": {
        "id": "v32Lb6YW5FNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "decaa292-8637-43cc-ace7-7d986a6bf369"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-25 07:33:54--  https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/rasters/beiluhe_google_img_201211_clip.tif\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17152742 (16M) [application/octet-stream]\n",
            "Saving to: ‘beiluhe_google_img_201211_clip.tif’\n",
            "\n",
            "beiluhe_google_img_ 100%[===================>]  16.36M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-12-25 07:33:55 (221 MB/s) - ‘beiluhe_google_img_201211_clip.tif’ saved [17152742/17152742]\n",
            "\n",
            "--2025-12-25 07:33:55--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.249.182.62, 13.249.182.39, 13.249.182.81, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.249.182.62|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1249524607 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_l_0b3195.pth’\n",
            "\n",
            "sam_vit_l_0b3195.pt 100%[===================>]   1.16G   198MB/s    in 6.5s    \n",
            "\n",
            "2025-12-25 07:34:01 (183 MB/s) - ‘sam_vit_l_0b3195.pth’ saved [1249524607/1249524607]\n",
            "\n",
            "--2025-12-25 07:34:01--  https://raw.githubusercontent.com/coolzhao/Geo-SAM/main/GeoSAM-Image-Encoder/examples/data/setting.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 494 [text/plain]\n",
            "Saving to: ‘setting.json’\n",
            "\n",
            "setting.json        100%[===================>]     494  --.-KB/s    in 0s      \n",
            "\n",
            "2025-12-25 07:34:02 (32.0 MB/s) - ‘setting.json’ saved [494/494]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage\n",
        "\n",
        "There are **two ways** to use GeoSAM-Image-Encoder. You can call it in Python or Terminal. We recommend using Python interface directly which will have greater flexibility."
      ],
      "metadata": {
        "id": "ILKiN60dXhQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Python\n",
        "\n",
        "After install GeoSAM-Image-Encoder, you can import it using `geosam`"
      ],
      "metadata": {
        "id": "96Dof82l31rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geosam\n",
        "from geosam import ImageEncoder"
      ],
      "metadata": {
        "id": "Z0K8RQV63H_v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if gpu available\n",
        "geosam.gpu_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwvXGZLYS5LZ",
        "outputId": "70117362-edb7-439d-c13e-fdf87b09a286"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by specify parameters directly\n",
        "\n",
        "If you want to specify the parameters directly, you can run it like this:"
      ],
      "metadata": {
        "id": "gb03VNJe4O2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/sam_vit_l_0b3195.pth'\n",
        "image_path = '/content/beiluhe_google_img_201211_clip.tif'\n",
        "feature_dir = './'\n",
        "\n",
        "## init ImageEncoder\n",
        "img_encoder = ImageEncoder(checkpoint_path)\n",
        "## encode image\n",
        "img_encoder.encode_image(image_path,feature_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNwD3v3D8RD1",
        "outputId": "d627c917-1736-442c-c698-61199e762c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:20<00:00,  1.70s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Run by parameters from setting.json file\n",
        "\n",
        "If you want to using `settings.json` file which exported from Geo-SAM plugin to provide parameters, you can run it like this:"
      ],
      "metadata": {
        "id": "SuHYf5BQTT1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setting_file = \"/content/setting.json\"\n",
        "feature_dir = './'\n",
        "\n",
        "### parse settings from the setting,json file\n",
        "settings = geosam.parse_settings_file(setting_file)\n",
        "\n",
        "### setting file not contains feature_dir, you need add it\n",
        "settings.update({\"feature_dir\":feature_dir})\n",
        "\n",
        "### split settings into init_settings, encode_settings\n",
        "init_settings, encode_settings = geosam.split_settings(settings)\n",
        "\n",
        "print(f\"settings: {settings}\")\n",
        "print(f\"init_settings: {init_settings}\")\n",
        "print(f\"encode_settings: {encode_settings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4r3xTViM91YL",
        "outputId": "7b9fefbf-f94c-45b0-d58f-8380be69a1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0, 'feature_dir': './'}\n",
            "init_settings: {'checkpoint_path': '/content/sam_vit_l_0b3195.pth', 'model_type': 1, 'batch_size': 1, 'gpu_id': 0}\n",
            "encode_settings: {'image_path': '/content/beiluhe_google_img_201211_clip.tif', 'bands': [1, 1, 1], 'value_range': '0.0,255.0', 'extent': '471407.9709, 473331.8546, 3882162.2353, 3884389.1008 [EPSG:32646]', 'resolution': 0.9999395530145561, 'stride': 512, 'feature_dir': './'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Then, you can run image encoding by parameters from setting.json file\n",
        "img_encoder = ImageEncoder(**init_settings)\n",
        "img_encoder.encode_image(**encode_settings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7fMTVTtFyjb",
        "outputId": "5fc4cd7b-401a-4197-8d3f-e973de1b855e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: (0.0, 255.0) (set by user)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '1', '1']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: (471407.9709, 473331.8546, 3882162.2353, 3884389.1008)\n",
            " Processing image size: (width 3410960, height 3411263)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '1', '1'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding image: 100%|██████████| 12/12 [00:13<00:00,  1.11s/batch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Output feature path\": .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Terminal\n",
        "\n",
        "Since this is a Colab example, Python will be used to demonstrate running it in the terminal."
      ],
      "metadata": {
        "id": "0YiFNWuz4iWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "## change cwd to geosam folder\n",
        "os.chdir(geosam.folder)\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDr4oyCKHMpV",
        "outputId": "7c86845d-006b-4be5-b2f1-b670e9510d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## get the command for terminal\n",
        "cmd = f\"image_encoder.py -i {image_path} -c {checkpoint_path} -f {feature_dir}\"\n",
        "print(cmd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD686kzAXR6G",
        "outputId": "d6efe171-366a-4d4a-c10a-490f59312d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_encoder.py -i /content/beiluhe_google_img_201211_clip.tif -c /content/sam_vit_l_0b3195.pth -f ./\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## run in terminal\n",
        "!python image_encoder.py -i /content/beiluhe_google_img_201211_clip.tif -c /content/sam_vit_l_0b3195.pth -f ./\n",
        "\n",
        "## You can overwrite the settings from file by specify the parameter values. For Example:\n",
        "# !python image_encoder.py -s /content/setting.json  -f ./ --stride 256 --value_range \"10,255\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_LbTprzXa0M",
        "outputId": "7d264629-c1a5-4c89-feec-b11736831198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "settings:\n",
            " {'feature_dir': PosixPath('/usr/local/lib/python3.10/dist-packages/geosam'), 'image_path': PosixPath('/content/beiluhe_google_img_201211_clip.tif'), 'checkpoint_path': PosixPath('/content/sam_vit_l_0b3195.pth'), 'stride': 512, 'batch_size': 1, 'gpu_id': 0}\n",
            "\n",
            "Initializing SAM model...\n",
            "\n",
            "\n",
            "----------------------------------------------\n",
            "     Start encoding image to SAM features\n",
            "----------------------------------------------\n",
            "\n",
            "Input Parameters:\n",
            "----------------------------------------------\n",
            " Input data value range to be rescaled: [0, 255] (automatically set based on min-max value of input image inside the processing extent.)\n",
            " Image path: /content/beiluhe_google_img_201211_clip.tif\n",
            " Bands selected: ['1', '2', '3']\n",
            " Target resolution: 0.9999395530145561\n",
            " Processing extent: [471407.9709, 3882162.2353, 473331.8546, 3884389.1008]\n",
            " Processing image size: (width 1924, height 2227)\n",
            "----------------------------------------------\n",
            "\n",
            "\n",
            "RasterDataset info \n",
            "----------------------------------------------\n",
            " filename_glob: beiluhe_google_img_201211_clip.tif, \n",
            " all bands: ['1', '2', '3', '4'], \n",
            " input bands: ['1', '2', '3'], \n",
            " resolution: 0.9999395530145561, \n",
            " bounds: [471407.9709, 473331.8546571067, 3882162.2353493366, 3884389.1008, 0.0, 9.223372036854776e+18], \n",
            " num: 1\n",
            "----------------------------------------------\n",
            "\n",
            "----------------------------------------------\n",
            " SAM model initialized. \n",
            "  SAM model type:  vit_l\n",
            " Device type: cuda:0\n",
            " Patch size: (1024, 1024) \n",
            " Batch size: 1\n",
            " Patch sample num: 12\n",
            " Total batch num: 12\n",
            "----------------------------------------------\n",
            "\n",
            "Encoding image: 100% 12/12 [00:13<00:00,  1.16s/batch]\n",
            "\"Output feature path\": /usr/local/lib/python3.10/dist-packages/geosam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## check all available parameters:\n",
        "!python image_encoder.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aXYVNWVyUbg",
        "outputId": "5da59b2f-dc24-46fa-f6aa-b0bc6ae13097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This script is for encoding image to SAM features.\n",
            "\n",
            "=====\n",
            "Usage\n",
            "=====\n",
            "using settings.json:\n",
            "\n",
            "    image_encoder.py -s <settings.json> -f <feature_dir>\n",
            " \n",
            " \n",
            "or directly using parameters:\n",
            " \n",
            "    image_encoder.py -i <image_path> -c <checkpoint_path> -f <feature_dir>\n",
            "    \n",
            "All Parameters:\n",
            "-------------------\n",
            "-s, --settings:         Path to the settings json file.\n",
            "-i, --image_path:       Path to the input image.\n",
            "-c, --checkpoint_path:  Path to the SAM checkpoint.\n",
            "-f, --feature_dir:      Path to the output feature directory.\n",
            "--model_type: one of [\"vit_h\", \"vit_l\", \"vit_b\"] or [0, 1, 2] or None, optional\n",
            "    The type of the SAM model. If None, the model type will be \n",
            "    inferred from the checkpoint path. Default: None. \n",
            "--bands: list of int, optional .\n",
            "    The bands to be used for encoding. Should not be more than three bands.\n",
            "    If None, the first three bands (if available) will be used. Default: None.\n",
            "--stride: int, optional\n",
            "    The stride of the sliding window. Default: 512.\n",
            "--extent: str, optional\n",
            "    The extent of the image to be encoded. Should be in the format of\n",
            "    \"minx, miny, maxx, maxy, [crs]\". If None, the extent of the input\n",
            "    image will be used. Default: None.\n",
            "--value_range: tuple of float, optional\n",
            "    The value range of the input image. If None, the value range will be\n",
            "    automatically calculated from the input image. Default: None.\n",
            "--resolution: float, optional\n",
            "    The resolution of the output feature in the unit of raster crs.\n",
            "    If None, the resolution of the input image will be used. Default: None.\n",
            "--batch_size: int, optional\n",
            "    The batch size for encoding. Default: 1.\n",
            "--gpu_id: int, optional\n",
            "    The device id of the GPU to be used. Default: 0.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}